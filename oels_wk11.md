---
title: Week 11 reading/practical
description: How to set up a server, launch and pay participants, manage qualifications
---

## The plan for week 11

We are basically done with the course - by this point you have read a bunch of papers using online data collection, and had the opportunity to play with jsPsych demo experiments to give you a feeling for how those experiments look under the hood (plus, hopefully, some stuff to get you started building your own experiments). But before you can actually *run* your experiment and collect data from real participants in the wild, there are a number of other steps you have to go through to make your experiment available on the open web and start collecting actual data. In this document I will talk you through those steps, and also give some advice based on my experience of running stuff online.

**Important note:** We are not doing actual data collection as part of this course, so you do not have to go through any of these steps for the course assessments. The closest we get is building an experiment for assignment 2, but you should **not** collect any data - we haven't gone through the School ethics process, which is an essential check that you must always complete before collecting data.

## Getting your experiment online

Once you have built your experiment and tested that it delivers the stims you think it does, records the responses you want, and seems reasonably robust to participants clicking in unexpected ways and mashing the keyboard, you will want to put it out into the wild and collect some real data. Before you can do that, there are a number of steps you need to go through:

1. Make sure your instructions are participant-friendly.
2. Get ethics!
3. Get a server to host your experiment.
4. Set up an account with a crowdsourcing site (e.g. MTurk, Prolific).
5. Put your experiment on that site, ad handle participant payment.

I'll step through each of these in turn.

## 1. Make sure your instructions are participant-friendly

People need to understand what they are supposed to be doing in your experiment, which means you need to provide them with proper pre-experiment information (e.g. telling them roughly what they'll be doing, how long it will take, and how they will be paid) so they can provide informed consent, but also good step-by-step instructions on how to complete your task. I think writing good instructions is actually quite tricky, which is why I've been quite lazy for the demo experiments here and just put in placeholder information and instruction screens.

I think writing instructions is tricky because you need to be relatively concise (if you give participants a wall of text they are less likely to read it) and avoid inappropriate technical vocabulary (in most cases our target participants are probably not linguists or psychologists) while still giving them a good sense of what's expected - what will happen on a trial, what am I as a participant supposed to do, and is there some criterion I have to meet to be allowed to continue (e.g. a minimal accuracy rate or something)? Getting other people's input on your instructions (e.g. getting a friend or colleague to look them over) can really help in ironing out any unclear bits or missing steps - what's obvious to you may not be obvious to a naive reader. I think images in instructions are always really helpful too - rather than saying "you will see two images and must click a button below the image with with the red square", a screenshot with text annotations ("you see two images", "here's the red square", "you should click this button") will make it crystal clear and avoid the wall-of-text that people assume is just more yada yada.

As part of the same process it's also worth sanitising your experiment more generally, so that you are not exposing information you don't want participants to see. Make sure the experiment title is participant-friendly: all the demo experiments have titles, set in the .html file, that are things like "iterated learning experiment" - unless you want to draw your participants' attention to the fact that it's an iterated learning experiment and have them wonder what that means, it might be better to change it to something more neutral, e.g. "language learning experiment". You might also find that in the course of building and debugging your experiment you end up with a bunch of stuff being logged in the console - participants can open the console too, so don't include anything in there that you don't want them to see.

## 2. Get ethics!

You cannot collect data from human participants without going through a process for obtaining ethical approval. In PPLS this involves submitting an application explaining the purpose of the experiment, who the participants are, what they will be paid, what kind of data will be collected, who will have access to their data, etc etc; you also provide the information sheets/screens that participants see in order to provide informed consent. This process is obviously crucial - it protects potential participants and their data, but the additional layer of scrutiny can be very useful in helping you spot problems with your participant information materials, data management plans and things like that.

## 3. Get a server to host your experiment

We have been testing running stuff out on jspsychlearning.ppls.ed.ac.uk, but that server isn't suitable for real data collection - apart from anything else you have to be on the University network to access it, which your participants probably won't be. You therefore need an externally accessible server, that is reliable (i.e. won't fall over at random times when you are trying to collect data, generating emails from irate participants) and that has the technical features you need (e.g. PHP for saving data, a security certificate if you want to record audio, python and web sockets if you are doing something with dyadic interaction). There are several optins you can look at.

### Edinburgh University hosting

The University provides free [web hosting](https://www.ed.ac.uk/information-services/computing/audio-visual-multi-media/web-hosting/hosting-service-options) that is suitable for our purposes in most cases - there are different levels, but they all support a basic webpage (with html, javascript) and PHP (for recording data etc), and they provide a security certificate (your webpage will be reached via https rather than plain http) so the audio recording stuff we covered in the confederate priming practical will work without any messing about with Chrome settings. You request that one of these servers be set up and you'll get to pick your own domain name (e.g. it would be something like https://kennyexperiments.ppls.ed.ac.uk); they send you login details and you should be able to connect via cyberduck, or you can use a browser-based tool they provide called cPanel (looks a bit like cyberduck, runs in a web browser, allows you to upload files etc). The only thing this *can't* do is websockets, which we used for dyadic interaction; the settings are standardised and quite tightly locked down for security.

### PPLS hosting  

PPLS also do some web-hosting, for stuff that can't be handled under the central University hosting - the jspsychlearning server is run by our PPLS technical people, but they also run other servers which are available on the open web (e.g. I run my stuff on a server called blake4). If you need something fancy (e.g. websockets for a dyadic interaction experiment) then you can request an account on one of our PPLS-hosted servers; because it's administered by our people they have a bit more flexibility in terms of what they can set up on the server for you.

### Hosting options aimed at jsPsych users

Obviously increasing numbers of people want to make web experiments available online, and not everyone has access to the hosting options Edinburgh University provides. There are therefore several services you can use, either for a fee or for free, that will allow you to make an experiment available online without getting fully involved in the challenges of setting up your own server. The jsPsych documentation on github [provides some options](https://github.com/jspsych/jsPsych/blob/master/docs/overview/collecting-data.md). Of those, [Cognition.run](https://www.cognition.run) and [Pavlovia](https://pavlovia.org) look like nice options if you aren't trying to do anything too fancy; [Psiturk](http://psiturk.org) and [Pushkin](https://languagelearninglab.gitbook.io/pushkin/) look more powerful but also more complex, and the set-up looks substantially more challenging (and with relatively sparse documentation, although I have only skimmed it).

[Cognition.run](https://www.cognition.run) is free and designed to work with jsPsych - you create a task (an experiment), drop in some jsPsych code into a web-based editor and it wraps that in an html page and gives you a public URL that participants can go to; it also saves data automatically to a CSV file that you can download from the task control panel. It looks *very* easy to use: basically you can take the contents of one of our .js experiment files, drop it into the "Task code" window, upload any images you need under the Stimuli link, and it will hopefully work. For instance, the word learning experiment works essentially as-is: I created a new task, called it "word learning test", opened the "Source code link", copied and pasted the contents of `word_learning.js` into the Task Code window, uploaded the 6 object image files under Stimuli, and got [a link](https://pzl0p84jyy.cognition.run/) which gives a working version of the experiment; I ran through it and some data appeared in my Results tab. There are some downsides though. It looks like anything we try to do with PHP will fail, which means no PHP scripts for recording audio or iterated learning (and obviously no python server for dyadic interaction, which is a whole other set of technology). "How do I save data if I can't use PHP?" you might ask - well, it saves the data for you, I can't see what the underlying mechanism is but it looks like it saves in real time, i.e. trial by trial, which is nice. *But* it's the full undigested psPsych data that gets saved (i.e. all the columns for all the trials, including instruction screens etc, with no option for specificity), so you'll have to hack out the data you want at the end. Second, using custom plugins might be a problem - it actually auto-loads any plugins you are using (which is clever), there is an option to upload external javascript files but I haven't tested whether e.g. the label-building or multiple-click plugins we used will work (although it may be possible to create similar effects using standard jsPsych plugins). Third, it's a free service, and it's actually not entirely clear from the webpage who set it up, why, or how long they plan to maintain it; I wouldn't count on it being around forever or not degrading. Finally, and importantly, you don't know exactly where the data is being stored or who has access, which will need to be covered in your ethics application (and I think would preclude storing any kind of sensitive or personal data, e.g. you don't want to collect MTurk or Prolific worker IDs).

[Pavlovia](https://pavlovia.org) looks like the same sort of idea: you upload your jsPsych code, they give you a URL for your experiment, and there's a bunch of back-end stuff going on to save your data. Pavlovia requires a paid account and you eat up credits every time someone runs the experiment (looks like £0.20 per participant). Many of the downsides are the same as for Cognition.run (no PHP as far as I can tell; only saves the default data *and* it looks like it saves once at the end of the experiment, not trial by trial; slight uncertainty about who has access to the data your participants generate); it also has some extra downsides (payment, saving data at the end, but also the interface for loading your experiment involves going through github which is an extra thing to figure out). The considerable upside is that Pavlovia is run by the people behind PsychoPy (a python library for behavioural experiments), they are based at Nottingham and now funded by the Welcome Trust, plus a bunch of UK Universities have Pavlovia subscriptions, so that plus that fact that it's pay-to-play makes me think it's likely to be around and maintained for a good while. If you are interested in checking out Pavlovia, go to the site, click on "Explore", filter the list of experimnets that comes up so that you can only see jsPsych experiments, and then look at the source code for e.g. the simple reaction time experiment - ypu can ee behind the scenes that it's a fairly familiar jsPsych experiment (although the javascript code for this example is all in index.html), but there are two extra trials in the timeline using an extra plugin type called `pavlovia` which connects the disconnects the experiment from the Pavlovia interface (presumably for data collection).

### Commercial hosting

Another option is to use a commercial company who can host your website. The  challenge is that we need something a bit more than what's called "static hosting", where you can just put html and js files that people can load into their browsers - we also need to be able to run PHP scripts (which involves the host server doing more than just passing on the source files for a webpage).

**In progress - I am working on how to do this!**

### 4. Set up an account with a crowdsourcing site (e.g. MTurk, Prolific)

Once you have your hosting set up, you have a URL for your experiment that you can direct your participants to - the next step is to get that link to your participants. If you are recruiting e.g. from friends or on social media then you are basically done - you send your participants the URL, they go to it and do the experiment. But if you are planning to make your experiment available on a crowdsourcing site, you'll need to create an account. Your two main options are [Amazon Mechanical Turk](https://www.mturk.com) and [Prolific](https://www.prolific.co) - see my notes from the first class on some of the features of these platforms. If you are wondering which to use from a technical perspective, my impression is that Prolific is a *lot* more user-friendly - everything is browser-based, you click around in a browser to set up your experiment and interact with participants etc and there is customer support available (I have only had to contact them once, to transfer some balance to a colleague, and they replied fast); MTurk is frankly fairly horrendous to deal with, poorly documented and no customer service *but* it probably has a larger participant pool and there are some nice ways of interacting with it programatically (see below for remarks on launching experiments and paying participants via python using boto3). If you are looking for an easy life I would suggest using Prolific.

To create a Prolific account, just go to their website and click SIGN UP.

To create an MTurk Requester account (requesters are people who are looking for workers to perform tasks, so we are requesters and our participants are workers), you can go to their website, click the "getting started" link, and create a requester account. *However*, you will also need requester and worker accounts on their test environment (the sandbox) so you can test out the infrastructure without accidentally spending money; if you also want to go down the route of using python to interact with mturk you will need some other bits and bobs too, so all in all it might be worth following steps 1-5 on their [Setting up accounts and tools](https://docs.aws.amazon.com/AWSMechTurk/latest/AWSMechanicalTurkGettingStartedGuide/SetUp.html) page - this will involve creating an AWS account (step 1), an MTurk requester account (step 2), linking them (step 3), creating an IAM user (step 4), and creating requester and worker accounts on the sandbox (step 5) and linking the sandbox requester account to your AWS account (also step 5). If you do all of that, make a careful note of the access key ID and secret access key you created under step 3, we will need them later.

Regardless of which platform you choose, you will need to load some credit onto your account, which involves pre-paying some amount of cash on there that covers your participant payments plus the cut that the platform takes (33% for Prolific, 20-40% for MTurk).

## Re-use

All aspects of this work are licensed under a [Creative Commons Attribution 4.0 International License](http://creativecommons.org/licenses/by/4.0/).
